{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperando embeddings com o pacote [Minicons](https://github.com/kanishkamisra/minicons)"
      ],
      "metadata": {
        "id": "uJdBE99xIMNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bjnHI-WWIb3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minicons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8KUXoYgIL7_",
        "outputId": "340240c0-f382-4ed3-e03a-d3fcdea18587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minicons\n",
            "  Downloading minicons-0.2.49-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from minicons) (0.34.2)\n",
            "Collecting openai<0.29.0,>=0.28.0 (from minicons)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pandas==2.2.0 (from minicons)\n",
            "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from minicons) (10.4.0)\n",
            "Collecting tenacity<9.0.0,>=8.2.3 (from minicons)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from minicons) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from minicons) (4.44.2)\n",
            "Collecting urllib3<2.0.0,>=1.26.7 (from minicons)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wonderwords<3.0.0,>=2.2.0 (from minicons)\n",
            "  Downloading wonderwords-2.2.0-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.0->minicons) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.0->minicons) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.0->minicons) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.0->minicons) (2024.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->minicons) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->minicons) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->minicons) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->minicons) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->minicons) (0.4.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.29.0,>=0.28.0->minicons) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.29.0,>=0.28.0->minicons) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.29.0,>=0.28.0->minicons) (3.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->minicons) (2024.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->minicons) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->minicons) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.0->minicons) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.29.0,>=0.28.0->minicons) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.29.0,>=0.28.0->minicons) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.29.0,>=0.28.0->minicons) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.29.0,>=0.28.0->minicons) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->minicons) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->minicons) (1.3.0)\n",
            "Downloading minicons-0.2.49-py3-none-any.whl (35 kB)\n",
            "Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wonderwords-2.2.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wonderwords, urllib3, tenacity, pandas, openai, minicons\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vamos começar a usar modelos disponíveis no [Hugging Face](https://huggingface.co/). O modelo abaixo é um BERT treinado em português, o [BERTimbau](https://huggingface.co/neuralmind/bert-base-portuguese-cased)"
      ],
      "metadata": {
        "id": "wCig84f5eJ54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mesma palavra, diferentes embeddings: conseguem entender a razão?"
      ],
      "metadata": {
        "id": "C1fsjFateApt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC00gwLDHu7Q"
      },
      "outputs": [],
      "source": [
        "from minicons import cwe\n",
        "\n",
        "model = cwe.CWE('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "context_words = [(\"Eu peguei o banco.\", \"banco\"),\n",
        "                 (\"E o banco era amarelo.\", \"banco\")]\n",
        "\n",
        "t1, t2 = model.extract_representation(context_words) # extraindo os vetores da palavra 'banco' da última camada do modelo\n",
        "print(t1.shape, t2.shape)\n",
        "\n",
        "print(model.extract_representation(context_words)) # veja que diferem abaixo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculando as similaridades. Primeiro entre a mesma palavra em duas frases"
      ],
      "metadata": {
        "id": "M5_0wBQdNQn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "Wbhe21biMOmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)"
      ],
      "metadata": {
        "id": "EkDoaAkSjxf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(' similaridade entre eles: ',cos(t1, t2))\n"
      ],
      "metadata": {
        "id": "hzcsMHe2MMil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similaridade entre duas palavras distintas. Observe o valor de similaridade"
      ],
      "metadata": {
        "id": "nFK0W6Zteyhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_words = [(\"Eu sentei no banco.\", \"banco\"),\n",
        "                 (\"Aquele banco que sentei quebrou.\", \"sentei\")]\n",
        "\n",
        "t1, t2 = model.extract_representation(context_words)\n",
        "\n",
        "print(t1.shape, t2.shape)\n",
        "\n",
        "print(model.extract_representation(context_words, layer = 12))\n",
        "print(' similaridade entre eles: ',cos(t1, t2))\n"
      ],
      "metadata": {
        "id": "ORIbEQBjNA8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verificando score de contexto de uma palavra igual com sentido distinto (valor maior, menos relevante)"
      ],
      "metadata": {
        "id": "-JMWbaATvo9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minicons import scorer\n",
        "mlm_model = scorer.MaskedLMScorer('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "sentence = ['sentei no banco que estava no banco do brasil.']\n",
        "\n",
        "\n",
        "mlm_model.token_score(sentence, PLL_metric='within_word_l2r')\n",
        "\n"
      ],
      "metadata": {
        "id": "fgCT3GaPOKWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verificando embeddings de sentenças. O Traansformer é orientado a tokens, uma das formas de recuperar a representaçào da sentença é fazendo a média entre as representações de cada token."
      ],
      "metadata": {
        "id": "0IPn0xUsfbll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "9LPThASwK-r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similaridade entre duas sentenças que estão com as palavras em ordem distintas. Veja que o modelo usado abaixo tem uma saída menor que o BERT base (768) no estado escondido da última camada (384)"
      ],
      "metadata": {
        "id": "0eGH1rpBfs-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"hoje bem cedo fui ao banco e sentei no banco\", \"ao banco bem cedo fui hoje e no banco sentei\"]\n"
      ],
      "metadata": {
        "id": "-vncV9ftvfWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "print('Tamanho dos embeddings: ', embeddings.shape)\n",
        "print(cos(torch.tensor(embeddings[0]), torch.tensor(embeddings[1])))"
      ],
      "metadata": {
        "id": "TCxVKGBzufiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ABW_uv1_h_P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agora a similaridade com o BERTimbau, fazendo a média explícita dos tokens"
      ],
      "metadata": {
        "id": "B8PWNVdtf8TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "CcovLUjYiWzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Carregando modelo e tokenizador explicitamente do HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')"
      ],
      "metadata": {
        "id": "Yvoxsu10iUtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "# attention_mask: define os tokens válidos e os que foram adicionados por padding\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #O primeiro elemento contém  os embeddings de cada token\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()  #para garantir que a máscara tem o mesmo tamanho do token\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokeniza as sentenças e deixa as duas no mesmo tamanho, para serem processadas juntas\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "# Computar embeddings dos tokens\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# executar max pooling.\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "print(\"Sentence embeddings:\")\n",
        "print(sentence_embeddings)\n",
        "\n",
        "print('Tamanho dos embeddings: ', sentence_embeddings.shape)\n",
        "print('Similaridade: ', cos(torch.tensor(sentence_embeddings[0]), torch.tensor(sentence_embeddings[1])))\n"
      ],
      "metadata": {
        "id": "zpmpfCq6vZtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agora vamos ver a similaridade entre os embeddings de uma mesma palavra com sentido diferente em uma mesma frase, para a saída de cada bloco do Transfomer (BERT base tem 12 camadas)"
      ],
      "metadata": {
        "id": "v-RqJY7egqiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig\n",
        "config = BertConfig.from_pretrained('neuralmind/bert-base-portuguese-cased', output_hidden_states=True)\n",
        "model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased', config=config)\n",
        "\n",
        "sentence = \"hoje bem cedo fui ao banco pegar dinheiro e sentei no banco da praça\"\n",
        "\n",
        "# Tokeniza a primeira sentença apenas\n",
        "encoded_input = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "#['hoje', 'bem', 'cedo', 'fu', '##i', 'ao', 'banco', 'pegar', 'dinheiro', 'e', 'sente', '##i', 'no', 'banco', 'da', 'praça']\n",
        "# a primeira palavra banco é o sexto token e a segunda é o 13\n",
        "\n",
        "pos1 = tokens.index('banco')\n",
        "pos2 = tokens.index('banco', pos1+1)\n",
        "\n",
        "print(pos1, tokens[pos1], pos2, tokens[pos2])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xDMKRu1s-rgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Computar token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# a saída terá a dimensão das camadas e dos tokens\n",
        "for i in range(len(model_output.hidden_states)):\n",
        "  t1 = model_output.hidden_states[i][0][pos1]\n",
        "  t2 = model_output.hidden_states[i][0][pos2]\n",
        "  print(f\"A similaridade entre as palavras na camada {i} é {cos(t1,t2)}\")"
      ],
      "metadata": {
        "id": "YWxSCV-VhjFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}