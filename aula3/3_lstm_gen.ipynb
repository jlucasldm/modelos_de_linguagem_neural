{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/"
      ],
      "metadata": {
        "id": "WDPnzeCXekvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "_q6fC6XsdYsT",
        "outputId": "1d2cc299-ba51-4780-c72d-a5826c274482"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f39ef7649ad3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Acessando o googledrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#Acessando o googledrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "C5fq4azHd8b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/drive/MyDrive/disciplinas/LMs/domCasmurro.txt\""
      ],
      "metadata": {
        "id": "Y9gGl_8mhEzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "domcasmurro = open(file_name, encoding=\"utf-8\").read().lower()\n"
      ],
      "metadata": {
        "id": "iNSp_BaZdc7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# caracteres para inteiros\n",
        "chars = sorted(list(set(domcasmurro)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "n_chars = len(domcasmurro)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total de caracteres: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMkwCdt0dwt4",
        "outputId": "bc1e7f21-d99f-497b-e02d-40dfa7b10c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de caracteres:  371506\n",
            "Total Vocab:  66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = domcasmurro[i:i + seq_length]\n",
        "    seq_out = domcasmurro[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjuZvKhHeiSc",
        "outputId": "b194c922-312c-4cfb-f59d-0c9a324d1af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  371406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(dataY)\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52m8B0sXev1_",
        "outputId": "37d051e6-6b63-42d7-a018-6249b350ca49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([371406, 100, 1]) torch.Size([371406])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6F9mC0mPhjKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "class CharModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        # produce output\n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Xb7XgHVWe8_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoVfzFTKgmWo",
        "outputId": "0abd7663-7946-4f63-ac4c-e2d3157f18d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_epochs = 40\n",
        "batch_size = 128\n",
        "model = CharModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
        "\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCiY7YbpfBWD",
        "outputId": "204ac079-3f92-4a90-f479-4e04e25e95a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Cross-entropy: 1036151.9375\n",
            "Epoch 1: Cross-entropy: 1004840.7500\n",
            "Epoch 2: Cross-entropy: 979231.6250\n",
            "Epoch 3: Cross-entropy: 948604.6875\n",
            "Epoch 4: Cross-entropy: 923658.3125\n",
            "Epoch 5: Cross-entropy: 897364.5625\n",
            "Epoch 6: Cross-entropy: 875768.8750\n",
            "Epoch 7: Cross-entropy: 857871.4375\n",
            "Epoch 8: Cross-entropy: 840565.4375\n",
            "Epoch 9: Cross-entropy: 826666.6250\n",
            "Epoch 10: Cross-entropy: 813936.3750\n",
            "Epoch 11: Cross-entropy: 803080.6875\n",
            "Epoch 12: Cross-entropy: 791802.0625\n",
            "Epoch 13: Cross-entropy: 782245.3750\n",
            "Epoch 14: Cross-entropy: 775540.1250\n",
            "Epoch 15: Cross-entropy: 763406.1875\n",
            "Epoch 16: Cross-entropy: 755099.5625\n",
            "Epoch 17: Cross-entropy: 749421.0000\n",
            "Epoch 18: Cross-entropy: 743199.4375\n",
            "Epoch 19: Cross-entropy: 737123.6250\n",
            "Epoch 20: Cross-entropy: 733307.6875\n",
            "Epoch 21: Cross-entropy: 728101.0625\n",
            "Epoch 22: Cross-entropy: 721563.5625\n",
            "Epoch 23: Cross-entropy: 723922.5625\n",
            "Epoch 24: Cross-entropy: 714453.0000\n",
            "Epoch 25: Cross-entropy: 709574.5000\n",
            "Epoch 26: Cross-entropy: 708838.0000\n",
            "Epoch 27: Cross-entropy: 704558.6250\n",
            "Epoch 28: Cross-entropy: 697849.3750\n",
            "Epoch 29: Cross-entropy: 697495.3125\n",
            "Epoch 30: Cross-entropy: 697580.9375\n",
            "Epoch 31: Cross-entropy: 691346.3125\n",
            "Epoch 32: Cross-entropy: 686384.2500\n",
            "Epoch 33: Cross-entropy: 685667.5625\n",
            "Epoch 34: Cross-entropy: 684314.5000\n",
            "Epoch 35: Cross-entropy: 682460.6875\n",
            "Epoch 36: Cross-entropy: 683296.9375\n",
            "Epoch 37: Cross-entropy: 680158.1250\n",
            "Epoch 38: Cross-entropy: 683390.4375\n",
            "Epoch 39: Cross-entropy: 676355.3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cARdVoiyfw6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation using the trained model\n",
        "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# randomly generate a prompt\n",
        "seq_length = 100\n",
        "raw_text = open(file_name, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n",
        "\n",
        "model.eval()\n",
        "print('Prompt: \"%s\"' % prompt)\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        # format input array of int into PyTorch tensor\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        # generate logits as output from the model\n",
        "        prediction = model(x.to(device))\n",
        "        # convert logits into one character\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        # append the new character into the prompt for the next iteration\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "5vnJeRr2gU0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}