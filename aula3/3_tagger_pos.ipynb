{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code from https://notebook.community/sameersingh/uci-statnlp/tutorials/rnn_examples\n",
        "Dataset from https://github.com/UniversalDependencies/UD_English"
      ],
      "metadata": {
        "id": "MoTtxKogBssE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPeK_aDEG1Rt",
        "outputId": "d5d6113f-48ca-4380-f0fc-6158fcac4cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/disciplinas/nlp/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGZpnFeLHAdd",
        "outputId": "308fb64c-ba99-422b-8f3e-76606c33ea8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/disciplinas/nlp/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9PqEDV_HRmu",
        "outputId": "73816e9a-be31-4063-a4ca-65e2f8907e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatbot\t\t\thelena.txt\t\t       reddit-cleanjokes.csv\n",
            "checkpoint_gen\t\tlogs\t\t\t       reddit-cleanjokes.txt\n",
            "corona_NLP_test.csv\tmodel5.gensim\t\t       results\n",
            "corpus.pkl\t\tmodel5.gensim.expElogbeta.npy  runs\n",
            "data-translate\t\tmodel5.gensim.id2word\t       squad\n",
            "dictionary.gensim\tmodel5.gensim.state\t       tagger-udpos-model.pt\n",
            "en_ewt-ud-dev.conllu\tpos_tagger.final.pt\t       test-squad\n",
            "en_ewt-ud-test.conllu\tpos_tagger.pt\t\t       tm\n",
            "en_ewt-ud-train.conllu\tpreprocdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# text = What if Google Morphed Into GoogleOS?\n",
        "# 1\tWhat\twhat\tPRON\tWP\tPronType=Int\t0\troot\t0:root\t_ if\tif\tSCONJ\tIN\t_\t4\tmark\t4:mark\t3\tGoogle\tGoogle\tPROPN\tNNP\tNumber=Sing\t4\tnsubj\t4:nsubj\t_4\tMorphed\tmorph\tVERB\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t1\tadvcl\t1:advcl:if\t_5\tInto\tinto\tADP\tIN\t_\t6\tcase\t6:case\t_6\tGoogleOS\tGoogleOS\tPROPN\tNNP\tNumber=Sing\t4\tobl\t4:obl:into\tSpaceAfter=No7\t?\t?\tPUNCT\t.\t_\t4\tpunct\t4:punct\t_"
      ],
      "metadata": {
        "id": "EvjMDkKaB13B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocab\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "            iter: An iterable which produces sequences of tokens used to update\n",
        "                the vocabulary.\n",
        "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
        "            sos_token: (Optional) Token denoting the start of a sequence.\n",
        "            eos_token: (Optional) Token denoting the end of a sequence.\n",
        "            unk_token: (Optional) Token denoting an unknown element in a\n",
        "                sequence.\n",
        "        \"\"\"\n",
        "        self.max_size = max_size\n",
        "        self.pad_token = '<pad>'\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.unk_token = unk_token\n",
        "\n",
        "        # Add special tokens.\n",
        "        id2word = [self.pad_token]\n",
        "        if sos_token is not None:\n",
        "            id2word.append(self.sos_token)\n",
        "        if eos_token is not None:\n",
        "            id2word.append(self.eos_token)\n",
        "        if unk_token is not None:\n",
        "            id2word.append(self.unk_token)\n",
        "\n",
        "        # Update counter with token counts.\n",
        "        counter = Counter()\n",
        "        for x in iter:\n",
        "            counter.update(x)\n",
        "\n",
        "        # Extract lookup tables.\n",
        "        if max_size is not None:\n",
        "            counts = counter.most_common(max_size)\n",
        "        else:\n",
        "            counts = counter.items()\n",
        "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
        "        words = [x[0] for x in counts]\n",
        "        id2word.extend(words)\n",
        "        word2id = {x: i for i, x in enumerate(id2word)}\n",
        "\n",
        "        self._id2word = id2word\n",
        "        self._word2id = word2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._id2word)\n",
        "\n",
        "    def word2id(self, word):\n",
        "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
        "        Args:\n",
        "            word: Word to lookup.\n",
        "        Returns:\n",
        "            id: The integer id of the word being looked up.\n",
        "        \"\"\"\n",
        "        if word in self._word2id:\n",
        "            return self._word2id[word]\n",
        "        elif self.unk_token is not None:\n",
        "            return self._word2id[self.unk_token]\n",
        "        else:\n",
        "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
        "\n",
        "    def id2word(self, id):\n",
        "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
        "        Args:\n",
        "            id: Integer id of the word being looked up.\n",
        "        Returns:\n",
        "            word: The corresponding word.\n",
        "        \"\"\"\n",
        "        return self._id2word[id]"
      ],
      "metadata": {
        "id": "5xBc0FZ9B-0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class Annotation(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
        "        self.tokens = []\n",
        "        self.pos_tags = []\n",
        "\n",
        "\n",
        "class CoNLLDataset(Dataset):\n",
        "    def __init__(self, fname, max_exs=None):\n",
        "        \"\"\"Initializes the CoNLLDataset.\n",
        "        Args:\n",
        "            fname: The .conllu file to load data from.\n",
        "        \"\"\"\n",
        "        self.fname = fname\n",
        "        self.annotations = self.process_conll_file(fname, max_exs)\n",
        "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
        "                                 unk_token='<unk>')\n",
        "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
        "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
        "        return input, target\n",
        "\n",
        "    def process_conll_file(self, fname, max_exs):\n",
        "        # Read the entire file.\n",
        "        with open(fname, 'r') as f:\n",
        "            raw_text = f.read()\n",
        "        # Split into chunks on blank lines.\n",
        "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
        "        #print(chunks)\n",
        "        # Process each chunk into an annotation.\n",
        "        annotations = []\n",
        "        exs = 0\n",
        "        for chunk in chunks:\n",
        "          if not max_exs or exs < max_exs:\n",
        "            annotation = Annotation()\n",
        "            lines = chunk.split('\\n')\n",
        "            # Iterate over all lines in the chunk.\n",
        "            for line in lines:\n",
        "                # If line is empty ignore it.\n",
        "                if len(line)==0:\n",
        "                    continue\n",
        "                # If line is a commend ignore it.\n",
        "                if line[0] == '#':\n",
        "                    continue\n",
        "                # Otherwise split on tabs and retrieve the token and the\n",
        "                # POS tag fields.\n",
        "                fields = line.split('\\t')\n",
        "                annotation.tokens.append(fields[1])\n",
        "                annotation.pos_tags.append(fields[3])\n",
        "            if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
        "                annotations.append(annotation)\n",
        "          exs += 1\n",
        "        return annotations"
      ],
      "metadata": {
        "id": "v9bGFUHcDM-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnjzGBt-Dd4e",
        "outputId": "7cd1c736-aa75-4c42-e8ce-28db5a614e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatbot\t\t       logs\t\t\t      reddit-cleanjokes.txt\n",
            "checkpoint_gen\t       model5.gensim\t\t      results\n",
            "corona_NLP_test.csv    model5.gensim.expElogbeta.npy  runs\n",
            "corpus.pkl\t       model5.gensim.id2word\t      squad\n",
            "data-translate\t       model5.gensim.state\t      tagger-udpos-model.pt\n",
            "dictionary.gensim      pos_tagger.final.pt\t      test-squad\n",
            "en_ewt-ud-dev.conllu   pos_tagger.pt\t\t      tm\n",
            "en_ewt-ud-test.conllu  preprocdata\n",
            "helena.txt\t       reddit-cleanjokes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CoNLLDataset('en_ewt-ud-train.conllu')"
      ],
      "metadata": {
        "id": "S7F5uAgpDUUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, target = dataset[0]\n",
        "print('Example input: %s\\n' % input)\n",
        "print('Example target: %s\\n' % target)\n",
        "print('Translated input: %s\\n' % ' '.join(dataset.token_vocab.id2word(x) for x in input))\n",
        "print('Translated target: %s\\n' % ' '.join(dataset.pos_vocab.id2word(x) for x in target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGw4Rz_b347k",
        "outputId": "024b592f-3005-495e-b030-1403a9a9a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example input: [266, 16, 5249, 45, 294, 703, 1154, 4233, 10099, 595, 16, 10100, 4, 3, 6865, 35, 3, 6866, 10, 3, 498, 8, 6867, 4, 758, 3, 2224, 1605, 2]\n",
            "\n",
            "Example target: [9, 2, 9, 2, 7, 1, 3, 9, 9, 9, 2, 9, 2, 6, 1, 5, 6, 1, 5, 6, 1, 5, 9, 2, 5, 6, 7, 1, 2]\n",
            "\n",
            "Translated input: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .\n",
            "\n",
            "Translated target: PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def pad(sequences, max_length, pad_value=0):\n",
        "    \"\"\"Pads a list of sequences.\n",
        "    Args:\n",
        "        sequences: A list of sequences to be padded.\n",
        "        max_length: The length to pad to.\n",
        "        pad_value: The value used for padding.\n",
        "    Returns:\n",
        "        A list of padded sequences.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for sequence in sequences:\n",
        "        padded = sequence + [0]*(max_length - len(sequence))\n",
        "        out.append(padded)\n",
        "    return out\n",
        "\n",
        "\n",
        "def collate_annotations(batch):\n",
        "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
        "    # Get inputs, targets, and lengths.\n",
        "    inputs, targets = zip(*batch)\n",
        "    lengths = [len(x) for x in inputs]\n",
        "    # Sort by length.\n",
        "    sort = sorted(zip(inputs, targets, lengths),\n",
        "                  key=lambda x: x[2],\n",
        "                  reverse=True)\n",
        "    inputs, targets, lengths = zip(*sort)\n",
        "    # Pad.\n",
        "    max_length = max(lengths)\n",
        "    inputs = pad(inputs, max_length)\n",
        "    targets = pad(targets, max_length)\n",
        "    # Transpose.\n",
        "    inputs = list(map(list, zip(*inputs)))\n",
        "    targets = list(map(list, zip(*targets)))\n",
        "    # Convert to PyTorch variables.\n",
        "    inputs = Variable(torch.LongTensor(inputs))\n",
        "    targets = Variable(torch.LongTensor(targets))\n",
        "    lengths = Variable(torch.LongTensor(lengths))\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        lengths = lengths.cuda()\n",
        "    return inputs, targets, lengths"
      ],
      "metadata": {
        "id": "C__SUb9o463W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "for inputs, targets, lengths in DataLoader(dataset, batch_size=16, collate_fn=collate_annotations):\n",
        "    print('Inputs: %s\\n' % inputs.data)\n",
        "    print('Targets: %s\\n' % targets.data)\n",
        "    print('Lengths: %s\\n' % lengths.data)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEgctw6y65Kr",
        "outputId": "88bcd53f-bf24-4eaa-8d9a-8943679c65ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[   28,  1083,   266,    28,    30,   106,    68,   266,   499,   625,\n",
            "         10103,   121,  1212,    28,    28,   108],\n",
            "        [10106,     3,    16,  1713,  6874,  6878, 10115,    16,  1030,   106,\n",
            "            45, 10123,     8,  3581,  1081,  1606],\n",
            "        [   10,  5252,  5249,  4237,    11,    11,    46,  5249,  4239,  1712,\n",
            "           555,     4,    69,    60,    19,    54],\n",
            "        [  180,    19,    45,     8,    10,     3,   185,    45,    51,     8,\n",
            "          1849,  6874,    60,  1370,   159,    41],\n",
            "        [   11,   343,   294, 10118, 10125,   759,   138,  5253, 10121,     7,\n",
            "          2018,  3111,   159,    10,   450,    19],\n",
            "        [ 4234,   163,   703,  3111,   180,  1031,     8,  1154,     7, 10101,\n",
            "            12,     4,   450,     3,    44, 10111],\n",
            "        [    5,     5,  1154,  2018,     6,    10,     3,     7, 10122, 10102,\n",
            "            31,   151,    44, 10112,     3,     3],\n",
            "        [    3,   408,  4233,    12,    50,     3,  2755,   807,  3112,    32,\n",
            "            51,   219,   144,     6,   607,   582],\n",
            "        [  142,  1470, 10099,  2756,    52,   207,  1851,     8,     6,    22,\n",
            "         10104,  1714,   704,   595,     8,    21],\n",
            "        [ 1029,    10,   595,    51,  6875,     8,    72,     3,    66,  3580,\n",
            "            61,    60,     8,    16,    52,    66],\n",
            "        [    4,  6871,    16,  4238,  2225,  2756,    60, 10116,   738,   140,\n",
            "           172,  2469,     3,  1290,  2019,  3110],\n",
            "        [   57,     6, 10100,  1607,   905,    10,  6873,  5254,   231,  1469,\n",
            "          1080,   136,  5250, 10113,  1371,  2020],\n",
            "        [   25,  4235,     4,  2021,     4,     3,   320,  1852,    31,    14,\n",
            "          3581,    31,     8,     8,     2,     2],\n",
            "        [   48,    61,     3,    10,    72,  1372,    14,   132,    60,   154,\n",
            "          1370,    84,     3,     3,     0,     0],\n",
            "        [   22,  1155,  6865,  5255,    23,    29,     3, 10117,    20,     5,\n",
            "            10,    22, 10105,  1082,     0,     0],\n",
            "        [   62,  3581,    35,     4,    20,  3113,    86,    82,    63,   216,\n",
            "          1850, 10124,    49,     2,     0,     0],\n",
            "        [  309,     4,     3, 10119,     3,     5,   660,    10,   111,     2,\n",
            "             2,     2,     0,     0,     0,     0],\n",
            "        [ 5251,     9,  6866,    58,  6876,     3,    10,  5255,   234,   626,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [10107,    96,    10,    51,    14,  2226,   904,     2,     2,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [10108,    36,     3,    70,  6877,    16,     2,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [10109,    38,   498,     7, 10126,  1715,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [ 6868,   181,     8,   378,     6,     8,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [   35,     3,  6867,  2022,  5256,     3,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [    7,   705,     4,    10,   906,   321,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  362,    12,   758,     3,    69,     2,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  596,     3,     3,   207,    10,    27,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  513,  6872,  2224,     8,     2,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [    8,   683,  1605, 10120,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [    3,     5,     2,     2,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [10110, 10114,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [ 6869,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [  100,   555,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [   10,   409,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [    3,    78,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [ 6870,  4236,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0],\n",
            "        [    2,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "Targets: tensor([[ 6, 14,  9,  6,  2,  6,  4,  9,  5,  2,  9,  5, 13,  6,  6,  4],\n",
            "        [ 9,  6,  2,  1,  9,  1,  3,  2,  9,  6,  2,  9,  5,  1,  7,  3],\n",
            "        [ 5,  1,  9,  1,  8,  8,  4,  9,  1,  1,  7,  2,  4,  8,  8, 14],\n",
            "        [ 9,  8,  2,  5,  5,  6, 10,  2,  8,  5,  1,  9,  8,  3,  8,  4],\n",
            "        [ 8, 10,  7,  9,  7,  7, 10,  1,  3,  6,  3,  9,  8,  5,  3,  8],\n",
            "        [ 7,  3,  1,  9,  9,  1,  5,  3,  6,  7, 14,  2,  3,  6,  5,  3],\n",
            "        [ 5, 12,  3,  3, 11,  5,  6,  6,  7,  1,  4, 13,  5,  9,  6,  6],\n",
            "        [ 6,  3,  9, 14,  3,  6,  9,  1,  1,  8,  8,  1, 13, 11,  1,  1],\n",
            "        [ 9,  1,  9,  1,  6,  1,  1,  5, 11,  8,  3,  1,  1,  9,  5,  5],\n",
            "        [ 9, 14,  9,  8, 10,  5,  4,  6,  4,  3,  5,  8,  5,  2,  6,  4],\n",
            "        [ 2,  3,  2,  3,  7,  1,  8,  9,  1,  4, 13,  3,  6,  9,  1,  1],\n",
            "        [10, 11,  9, 13,  1,  5, 10,  7, 14,  1,  7, 14,  9,  1,  1,  1],\n",
            "        [ 4,  3,  2,  1,  2,  6,  3,  9,  4,  5,  1,  4,  5,  5,  2,  2],\n",
            "        [ 8,  5,  6,  5,  4,  1,  5, 14,  8,  1,  3,  8,  6,  6,  0,  0],\n",
            "        [ 8,  1,  1,  9,  8, 14,  6,  3,  5, 12,  5,  8,  9,  1,  0,  0],\n",
            "        [14,  1,  5,  2,  5,  3,  7,  4,  4,  3,  9,  3,  2,  2,  0,  0],\n",
            "        [ 3,  2,  6, 14,  6,  5,  1,  5,  1,  2,  2,  2,  0,  0,  0,  0],\n",
            "        [ 9,  4,  1,  4,  1,  6,  5,  9, 10,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 9, 15,  5,  8,  5,  1,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 9,  8,  6,  3,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [10, 12,  1,  6,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 3,  3,  5,  7, 11,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 5,  6,  9,  1, 10,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 6,  1,  2,  5,  3,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 7, 14,  5,  6,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  6,  6,  1,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  9,  7,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 5,  3,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 6, 12,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 9,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [10,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 5,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "\n",
            "Lengths: tensor([36, 36, 29, 29, 27, 26, 20, 19, 19, 18, 17, 17, 16, 16, 13, 13])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Tagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 embedding_dim=64,\n",
        "                 hidden_dim=64,\n",
        "                 dropout=0.5,\n",
        "                 bidirectional=True,\n",
        "                 pad_idx=0):\n",
        "        \"\"\"Initializes the tagger.\n",
        "\n",
        "        Args:\n",
        "            input_dim: Size of the input vocabulary, projection\n",
        "            output_dim: Size of the output vocabulary.\n",
        "            embedding_dim: Dimension of the word embeddings.\n",
        "            hidden_dim: Number of units in each LSTM hidden layer.\n",
        "            bidirectional: Whether or not to use a bidirectional rnn.\n",
        "        \"\"\"\n",
        "        super(Tagger, self).__init__()\n",
        "\n",
        "        # Store parameters\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Define layers\n",
        "        self.word_embeddings = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers,\n",
        "                          bidirectional=bidirectional,\n",
        "                          dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.activation = nn.LogSoftmax(dim=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths=None, hidden=None):\n",
        "        \"\"\"Computes a forward pass of the language model.\n",
        "\n",
        "        Args:\n",
        "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
        "            lengths: The lengths of the sequences in x.\n",
        "            hidden: Hidden state to be fed into the lstm.\n",
        "\n",
        "        Returns:\n",
        "            net: Probability of the next word in the sequence.\n",
        "            hidden: Hidden state of the lstm.\n",
        "        \"\"\"\n",
        "        seq_len, batch_size = x.size()\n",
        "\n",
        "        # If no hidden state is provided, then default to zeros.\n",
        "        if hidden is None:\n",
        "            if self.bidirectional:\n",
        "                num_directions = 2\n",
        "            else:\n",
        "                num_directions = 1\n",
        "            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_dim))\n",
        "            if torch.cuda.is_available():\n",
        "                hidden = hidden.cuda()\n",
        "\n",
        "        net = self.word_embeddings(x)\n",
        "        # Pack before feeding into the RNN.\n",
        "        if lengths is not None:\n",
        "            lengths = lengths.data.view(-1).tolist()\n",
        "            net = pack_padded_sequence(net, lengths)\n",
        "        net, hidden = self.rnn(net, hidden)\n",
        "        # Unpack after\n",
        "        if lengths is not None:\n",
        "            net, _ = pad_packed_sequence(net)\n",
        "        net = self.fc(net)\n",
        "        net = self.activation(net)\n",
        "\n",
        "        return net, hidden"
      ],
      "metadata": {
        "id": "-C9Nc3Gu9O-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load datasets.\n",
        "train_dataset = CoNLLDataset('en_ewt-ud-train.conllu', 4096)\n",
        "dev_dataset = CoNLLDataset('en_ewt-ud-dev.conllu', 1024)\n",
        "\n",
        "dev_dataset.token_vocab = train_dataset.token_vocab\n",
        "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
        "\n",
        "# Hyperparameters / constants.\n",
        "input_vocab_size = len(train_dataset.token_vocab)\n",
        "output_vocab_size = len(train_dataset.pos_vocab)\n",
        "batch_size = 16\n",
        "epochs = 6\n",
        "n_layers = 1\n",
        "\n",
        "# Initialize the model.\n",
        "model = Tagger(input_vocab_size, output_vocab_size, n_layers)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Loss function weights.\n",
        "weight = torch.ones(output_vocab_size)\n",
        "weight[0] = 0\n",
        "if torch.cuda.is_available():\n",
        "    weight = weight.cuda()\n",
        "\n",
        "# Initialize loss function and optimizer.\n",
        "loss_function = torch.nn.NLLLoss(weight)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Main training loop.\n",
        "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                         collate_fn=collate_annotations)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        collate_fn=collate_annotations)\n",
        "losses = []\n",
        "i = 0\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets, lengths in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs, lengths=lengths)\n",
        "\n",
        "        outputs = outputs.view(-1, output_vocab_size)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #losses.append(loss.data[0])\n",
        "        losses.append(loss.item())\n",
        "        if (i % 10) == 0:\n",
        "            # Compute dev loss over entire dev set.\n",
        "            # NOTE: This is expensive. You may want to only use a\n",
        "            # subset of the dev set.\n",
        "            #print('iteration, ', i)\n",
        "            dev_losses = []\n",
        "            for inputs, targets, lengths in dev_loader:\n",
        "                outputs, _ = model(inputs, lengths=lengths)\n",
        "                outputs = outputs.view(-1, output_vocab_size)\n",
        "                targets = targets.view(-1)\n",
        "                loss = loss_function(outputs, targets)\n",
        "                dev_losses.append(loss.item())\n",
        "            avg_train_loss = np.mean(losses)\n",
        "            avg_dev_loss = np.mean(dev_losses)\n",
        "            losses = []\n",
        "            #print('here')\n",
        "            print('Epoch %i Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (epoch, i, avg_train_loss, avg_dev_loss), end='\\n')\n",
        "            torch.save(model, 'pos_tagger.pt')\n",
        "        i += 1\n",
        "\n",
        "torch.save(model, 'pos_tagger.final.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKM0k8c6A8l6",
        "outputId": "9724cf75-81c4-4eb4-8e5d-da0d75dc9a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Iteration 0 - Train Loss: 2.944906 - Dev Loss: 2.940190\n",
            "Epoch 0 Iteration 10 - Train Loss: 2.845249 - Dev Loss: 2.734218\n",
            "Epoch 0 Iteration 20 - Train Loss: 2.640595 - Dev Loss: 2.531120\n",
            "Epoch 0 Iteration 30 - Train Loss: 2.448569 - Dev Loss: 2.325695\n",
            "Epoch 0 Iteration 40 - Train Loss: 2.232043 - Dev Loss: 2.138989\n",
            "Epoch 0 Iteration 50 - Train Loss: 2.071410 - Dev Loss: 1.997733\n",
            "Epoch 0 Iteration 60 - Train Loss: 1.869212 - Dev Loss: 1.873686\n",
            "Epoch 0 Iteration 70 - Train Loss: 1.820451 - Dev Loss: 1.767294\n",
            "Epoch 0 Iteration 80 - Train Loss: 1.650056 - Dev Loss: 1.681365\n",
            "Epoch 0 Iteration 90 - Train Loss: 1.591510 - Dev Loss: 1.595364\n",
            "Epoch 0 Iteration 100 - Train Loss: 1.475191 - Dev Loss: 1.525031\n",
            "Epoch 0 Iteration 110 - Train Loss: 1.425099 - Dev Loss: 1.475127\n",
            "Epoch 0 Iteration 120 - Train Loss: 1.367612 - Dev Loss: 1.422311\n",
            "Epoch 0 Iteration 130 - Train Loss: 1.361713 - Dev Loss: 1.376067\n",
            "Epoch 0 Iteration 140 - Train Loss: 1.284507 - Dev Loss: 1.335651\n",
            "Epoch 0 Iteration 150 - Train Loss: 1.216734 - Dev Loss: 1.306125\n",
            "Epoch 0 Iteration 160 - Train Loss: 1.245916 - Dev Loss: 1.276976\n",
            "Epoch 0 Iteration 170 - Train Loss: 1.165685 - Dev Loss: 1.245254\n",
            "Epoch 0 Iteration 180 - Train Loss: 1.211158 - Dev Loss: 1.219084\n",
            "Epoch 0 Iteration 190 - Train Loss: 1.118859 - Dev Loss: 1.194285\n",
            "Epoch 0 Iteration 200 - Train Loss: 1.090813 - Dev Loss: 1.170042\n",
            "Epoch 0 Iteration 210 - Train Loss: 1.117663 - Dev Loss: 1.152468\n",
            "Epoch 0 Iteration 220 - Train Loss: 1.040625 - Dev Loss: 1.133252\n",
            "Epoch 0 Iteration 230 - Train Loss: 0.982828 - Dev Loss: 1.116488\n",
            "Epoch 0 Iteration 240 - Train Loss: 1.051586 - Dev Loss: 1.092634\n",
            "Epoch 0 Iteration 250 - Train Loss: 1.079665 - Dev Loss: 1.078557\n",
            "Epoch 1 Iteration 260 - Train Loss: 0.995305 - Dev Loss: 1.063683\n",
            "Epoch 1 Iteration 270 - Train Loss: 0.946117 - Dev Loss: 1.045582\n",
            "Epoch 1 Iteration 280 - Train Loss: 0.936434 - Dev Loss: 1.035957\n",
            "Epoch 1 Iteration 290 - Train Loss: 0.856947 - Dev Loss: 1.025259\n",
            "Epoch 1 Iteration 300 - Train Loss: 0.925949 - Dev Loss: 1.010374\n",
            "Epoch 1 Iteration 310 - Train Loss: 0.867132 - Dev Loss: 1.003602\n",
            "Epoch 1 Iteration 320 - Train Loss: 0.855238 - Dev Loss: 0.985640\n",
            "Epoch 1 Iteration 330 - Train Loss: 0.828970 - Dev Loss: 0.974691\n",
            "Epoch 1 Iteration 340 - Train Loss: 0.826374 - Dev Loss: 0.963527\n",
            "Epoch 1 Iteration 350 - Train Loss: 0.887630 - Dev Loss: 0.953169\n",
            "Epoch 1 Iteration 360 - Train Loss: 0.826314 - Dev Loss: 0.945460\n",
            "Epoch 1 Iteration 370 - Train Loss: 0.800366 - Dev Loss: 0.931344\n",
            "Epoch 1 Iteration 380 - Train Loss: 0.806924 - Dev Loss: 0.922620\n",
            "Epoch 1 Iteration 390 - Train Loss: 0.788404 - Dev Loss: 0.912785\n",
            "Epoch 1 Iteration 400 - Train Loss: 0.816069 - Dev Loss: 0.909456\n",
            "Epoch 1 Iteration 410 - Train Loss: 0.808602 - Dev Loss: 0.898399\n",
            "Epoch 1 Iteration 420 - Train Loss: 0.815666 - Dev Loss: 0.886292\n",
            "Epoch 1 Iteration 430 - Train Loss: 0.771475 - Dev Loss: 0.876797\n",
            "Epoch 1 Iteration 440 - Train Loss: 0.772038 - Dev Loss: 0.871323\n",
            "Epoch 1 Iteration 450 - Train Loss: 0.810536 - Dev Loss: 0.869330\n",
            "Epoch 1 Iteration 460 - Train Loss: 0.767482 - Dev Loss: 0.858955\n",
            "Epoch 1 Iteration 470 - Train Loss: 0.715538 - Dev Loss: 0.860127\n",
            "Epoch 1 Iteration 480 - Train Loss: 0.697689 - Dev Loss: 0.856185\n",
            "Epoch 1 Iteration 490 - Train Loss: 0.776622 - Dev Loss: 0.841468\n",
            "Epoch 1 Iteration 500 - Train Loss: 0.699109 - Dev Loss: 0.833373\n",
            "Epoch 1 Iteration 510 - Train Loss: 0.676421 - Dev Loss: 0.834910\n",
            "Epoch 2 Iteration 520 - Train Loss: 0.717841 - Dev Loss: 0.830640\n",
            "Epoch 2 Iteration 530 - Train Loss: 0.697341 - Dev Loss: 0.817685\n",
            "Epoch 2 Iteration 540 - Train Loss: 0.619943 - Dev Loss: 0.819612\n",
            "Epoch 2 Iteration 550 - Train Loss: 0.634257 - Dev Loss: 0.814923\n",
            "Epoch 2 Iteration 560 - Train Loss: 0.665512 - Dev Loss: 0.802634\n",
            "Epoch 2 Iteration 570 - Train Loss: 0.676301 - Dev Loss: 0.797419\n",
            "Epoch 2 Iteration 580 - Train Loss: 0.636403 - Dev Loss: 0.786656\n",
            "Epoch 2 Iteration 590 - Train Loss: 0.597758 - Dev Loss: 0.781910\n",
            "Epoch 2 Iteration 600 - Train Loss: 0.584231 - Dev Loss: 0.779122\n",
            "Epoch 2 Iteration 610 - Train Loss: 0.635739 - Dev Loss: 0.769186\n",
            "Epoch 2 Iteration 620 - Train Loss: 0.585261 - Dev Loss: 0.766257\n",
            "Epoch 2 Iteration 630 - Train Loss: 0.602759 - Dev Loss: 0.761051\n",
            "Epoch 2 Iteration 640 - Train Loss: 0.547395 - Dev Loss: 0.756774\n",
            "Epoch 2 Iteration 650 - Train Loss: 0.590872 - Dev Loss: 0.760740\n",
            "Epoch 2 Iteration 660 - Train Loss: 0.595557 - Dev Loss: 0.764364\n",
            "Epoch 2 Iteration 670 - Train Loss: 0.563889 - Dev Loss: 0.758215\n",
            "Epoch 2 Iteration 680 - Train Loss: 0.530460 - Dev Loss: 0.752416\n",
            "Epoch 2 Iteration 690 - Train Loss: 0.566004 - Dev Loss: 0.743243\n",
            "Epoch 2 Iteration 700 - Train Loss: 0.579138 - Dev Loss: 0.742846\n",
            "Epoch 2 Iteration 710 - Train Loss: 0.547641 - Dev Loss: 0.741493\n",
            "Epoch 2 Iteration 720 - Train Loss: 0.534612 - Dev Loss: 0.736052\n",
            "Epoch 2 Iteration 730 - Train Loss: 0.554569 - Dev Loss: 0.728062\n",
            "Epoch 2 Iteration 740 - Train Loss: 0.577257 - Dev Loss: 0.727567\n",
            "Epoch 2 Iteration 750 - Train Loss: 0.563198 - Dev Loss: 0.732954\n",
            "Epoch 2 Iteration 760 - Train Loss: 0.529495 - Dev Loss: 0.730809\n",
            "Epoch 3 Iteration 770 - Train Loss: 0.525665 - Dev Loss: 0.724927\n",
            "Epoch 3 Iteration 780 - Train Loss: 0.499191 - Dev Loss: 0.726795\n",
            "Epoch 3 Iteration 790 - Train Loss: 0.521143 - Dev Loss: 0.726463\n",
            "Epoch 3 Iteration 800 - Train Loss: 0.484435 - Dev Loss: 0.720486\n",
            "Epoch 3 Iteration 810 - Train Loss: 0.446847 - Dev Loss: 0.714194\n",
            "Epoch 3 Iteration 820 - Train Loss: 0.519995 - Dev Loss: 0.716980\n",
            "Epoch 3 Iteration 830 - Train Loss: 0.504064 - Dev Loss: 0.706923\n",
            "Epoch 3 Iteration 840 - Train Loss: 0.472146 - Dev Loss: 0.704457\n",
            "Epoch 3 Iteration 850 - Train Loss: 0.476741 - Dev Loss: 0.704395\n",
            "Epoch 3 Iteration 860 - Train Loss: 0.439143 - Dev Loss: 0.699415\n",
            "Epoch 3 Iteration 870 - Train Loss: 0.452642 - Dev Loss: 0.700140\n",
            "Epoch 3 Iteration 880 - Train Loss: 0.458817 - Dev Loss: 0.697316\n",
            "Epoch 3 Iteration 890 - Train Loss: 0.475060 - Dev Loss: 0.690018\n",
            "Epoch 3 Iteration 900 - Train Loss: 0.440967 - Dev Loss: 0.689784\n",
            "Epoch 3 Iteration 910 - Train Loss: 0.439722 - Dev Loss: 0.682522\n",
            "Epoch 3 Iteration 920 - Train Loss: 0.466826 - Dev Loss: 0.679013\n",
            "Epoch 3 Iteration 930 - Train Loss: 0.450951 - Dev Loss: 0.685382\n",
            "Epoch 3 Iteration 940 - Train Loss: 0.439451 - Dev Loss: 0.688294\n",
            "Epoch 3 Iteration 950 - Train Loss: 0.446563 - Dev Loss: 0.683032\n",
            "Epoch 3 Iteration 960 - Train Loss: 0.487073 - Dev Loss: 0.675978\n",
            "Epoch 3 Iteration 970 - Train Loss: 0.429097 - Dev Loss: 0.671867\n",
            "Epoch 3 Iteration 980 - Train Loss: 0.390220 - Dev Loss: 0.672242\n",
            "Epoch 3 Iteration 990 - Train Loss: 0.423593 - Dev Loss: 0.669987\n",
            "Epoch 3 Iteration 1000 - Train Loss: 0.388918 - Dev Loss: 0.668701\n",
            "Epoch 3 Iteration 1010 - Train Loss: 0.437576 - Dev Loss: 0.670441\n",
            "Epoch 3 Iteration 1020 - Train Loss: 0.432602 - Dev Loss: 0.668394\n",
            "Epoch 4 Iteration 1030 - Train Loss: 0.374362 - Dev Loss: 0.665883\n",
            "Epoch 4 Iteration 1040 - Train Loss: 0.350939 - Dev Loss: 0.666123\n",
            "Epoch 4 Iteration 1050 - Train Loss: 0.393768 - Dev Loss: 0.668052\n",
            "Epoch 4 Iteration 1060 - Train Loss: 0.390621 - Dev Loss: 0.660045\n",
            "Epoch 4 Iteration 1070 - Train Loss: 0.378630 - Dev Loss: 0.652009\n",
            "Epoch 4 Iteration 1080 - Train Loss: 0.362558 - Dev Loss: 0.655206\n",
            "Epoch 4 Iteration 1090 - Train Loss: 0.347820 - Dev Loss: 0.656771\n",
            "Epoch 4 Iteration 1100 - Train Loss: 0.346564 - Dev Loss: 0.648893\n",
            "Epoch 4 Iteration 1110 - Train Loss: 0.392674 - Dev Loss: 0.646169\n",
            "Epoch 4 Iteration 1120 - Train Loss: 0.371485 - Dev Loss: 0.653653\n",
            "Epoch 4 Iteration 1130 - Train Loss: 0.333849 - Dev Loss: 0.650479\n",
            "Epoch 4 Iteration 1140 - Train Loss: 0.358038 - Dev Loss: 0.643861\n",
            "Epoch 4 Iteration 1150 - Train Loss: 0.368497 - Dev Loss: 0.642207\n",
            "Epoch 4 Iteration 1160 - Train Loss: 0.362013 - Dev Loss: 0.641925\n",
            "Epoch 4 Iteration 1170 - Train Loss: 0.299804 - Dev Loss: 0.639717\n",
            "Epoch 4 Iteration 1180 - Train Loss: 0.343357 - Dev Loss: 0.636963\n",
            "Epoch 4 Iteration 1190 - Train Loss: 0.337907 - Dev Loss: 0.644228\n",
            "Epoch 4 Iteration 1200 - Train Loss: 0.355384 - Dev Loss: 0.639219\n",
            "Epoch 4 Iteration 1210 - Train Loss: 0.358998 - Dev Loss: 0.635025\n",
            "Epoch 4 Iteration 1220 - Train Loss: 0.334694 - Dev Loss: 0.631712\n",
            "Epoch 4 Iteration 1230 - Train Loss: 0.371029 - Dev Loss: 0.641812\n",
            "Epoch 4 Iteration 1240 - Train Loss: 0.344169 - Dev Loss: 0.641639\n",
            "Epoch 4 Iteration 1250 - Train Loss: 0.328855 - Dev Loss: 0.631177\n",
            "Epoch 4 Iteration 1260 - Train Loss: 0.330044 - Dev Loss: 0.627503\n",
            "Epoch 4 Iteration 1270 - Train Loss: 0.292551 - Dev Loss: 0.631939\n",
            "Epoch 5 Iteration 1280 - Train Loss: 0.369836 - Dev Loss: 0.635653\n",
            "Epoch 5 Iteration 1290 - Train Loss: 0.271892 - Dev Loss: 0.635315\n",
            "Epoch 5 Iteration 1300 - Train Loss: 0.271700 - Dev Loss: 0.637603\n",
            "Epoch 5 Iteration 1310 - Train Loss: 0.286894 - Dev Loss: 0.638918\n",
            "Epoch 5 Iteration 1320 - Train Loss: 0.318641 - Dev Loss: 0.632433\n",
            "Epoch 5 Iteration 1330 - Train Loss: 0.289027 - Dev Loss: 0.629483\n",
            "Epoch 5 Iteration 1340 - Train Loss: 0.264265 - Dev Loss: 0.632328\n",
            "Epoch 5 Iteration 1350 - Train Loss: 0.298333 - Dev Loss: 0.622557\n",
            "Epoch 5 Iteration 1360 - Train Loss: 0.278132 - Dev Loss: 0.618015\n",
            "Epoch 5 Iteration 1370 - Train Loss: 0.274381 - Dev Loss: 0.621247\n",
            "Epoch 5 Iteration 1380 - Train Loss: 0.284574 - Dev Loss: 0.626232\n",
            "Epoch 5 Iteration 1390 - Train Loss: 0.305095 - Dev Loss: 0.623555\n",
            "Epoch 5 Iteration 1400 - Train Loss: 0.290958 - Dev Loss: 0.621512\n",
            "Epoch 5 Iteration 1410 - Train Loss: 0.316328 - Dev Loss: 0.624206\n",
            "Epoch 5 Iteration 1420 - Train Loss: 0.273076 - Dev Loss: 0.624976\n",
            "Epoch 5 Iteration 1430 - Train Loss: 0.260851 - Dev Loss: 0.627895\n",
            "Epoch 5 Iteration 1440 - Train Loss: 0.285283 - Dev Loss: 0.633629\n",
            "Epoch 5 Iteration 1450 - Train Loss: 0.271227 - Dev Loss: 0.631029\n",
            "Epoch 5 Iteration 1460 - Train Loss: 0.255365 - Dev Loss: 0.619939\n",
            "Epoch 5 Iteration 1470 - Train Loss: 0.280838 - Dev Loss: 0.612253\n",
            "Epoch 5 Iteration 1480 - Train Loss: 0.273629 - Dev Loss: 0.611604\n",
            "Epoch 5 Iteration 1490 - Train Loss: 0.261633 - Dev Loss: 0.612214\n",
            "Epoch 5 Iteration 1500 - Train Loss: 0.255287 - Dev Loss: 0.615240\n",
            "Epoch 5 Iteration 1510 - Train Loss: 0.284364 - Dev Loss: 0.604084\n",
            "Epoch 5 Iteration 1520 - Train Loss: 0.269672 - Dev Loss: 0.605015\n",
            "Epoch 5 Iteration 1530 - Train Loss: 0.257921 - Dev Loss: 0.607406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the predictions and targets\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for inputs, targets, lengths in dev_loader:\n",
        "    outputs, _ = model(inputs, lengths=lengths)\n",
        "    _, preds = torch.max(outputs, dim=2)\n",
        "    targets = targets.view(-1)\n",
        "    preds = preds.view(-1)\n",
        "    if torch.cuda.is_available():\n",
        "        targets = targets.cpu()\n",
        "        preds = preds.cpu()\n",
        "    y_true.append(targets.data.numpy())\n",
        "    y_pred.append(preds.data.numpy())\n",
        "\n",
        "# Stack into numpy arrays\n",
        "y_true = np.concatenate(y_true)\n",
        "y_pred = np.concatenate(y_pred)\n",
        "\n",
        "# Compute accuracy\n",
        "acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
        "print('Accuracy - %0.6f\\n' % acc)\n",
        "\n",
        "# Evaluate f1-score\n",
        "from sklearn.metrics import f1_score\n",
        "score = f1_score(y_true, y_pred, average=None)\n",
        "print('F1-scores:\\n')\n",
        "for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
        "    print('%s - %0.6f' % (label, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9f6s6pFkoBn",
        "outputId": "c7d67e05-b342-4788-c42c-cbfbe8056a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy - 0.816063\n",
            "\n",
            "F1-scores:\n",
            "\n",
            "NOUN - 0.765083\n",
            "PUNCT - 0.984068\n",
            "VERB - 0.781020\n",
            "ADP - 0.750791\n",
            "PROPN - 0.532544\n",
            "DET - 0.976991\n",
            "PRON - 0.960609\n",
            "ADJ - 0.684080\n",
            "AUX - 0.943653\n",
            "ADV - 0.703019\n",
            "CCONJ - 0.980344\n",
            "PART - 0.909847\n",
            "NUM - 0.661509\n",
            "SCONJ - 0.706927\n",
            "_ - 0.879121\n",
            "X - 0.000598\n",
            "INTJ - 0.765432\n",
            "SYM - 0.311111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('pos_tagger.final.pt')\n",
        "\n",
        "def inference(sentence):\n",
        "    # Convert words to id tensor.\n",
        "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
        "    ids = Variable(torch.LongTensor(ids))\n",
        "    if torch.cuda.is_available():\n",
        "        ids = ids.cuda()\n",
        "    # Get model output.\n",
        "    output, _ = model(ids)\n",
        "    _, preds = torch.max(output, dim=2)\n",
        "    if torch.cuda.is_available():\n",
        "        preds = preds.cpu()\n",
        "    preds = preds.data.view(-1).numpy()\n",
        "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
        "    for word, tag in zip(sentence, pos_tags):\n",
        "        print('%s - %s' % (word, tag))"
      ],
      "metadata": {
        "id": "HBnIJZsAkyLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_with_labels(sentence, labels):\n",
        "    #print(sentence)\n",
        "    # Convert words to id tensor.\n",
        "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
        "    print(ids)\n",
        "    ids = Variable(torch.LongTensor(ids))\n",
        "    if torch.cuda.is_available():\n",
        "        ids = ids.cuda()\n",
        "    # Get model output.\n",
        "    output, _ = model(ids)\n",
        "    _, preds = torch.max(output, dim=2)\n",
        "    if torch.cuda.is_available():\n",
        "        preds = preds.cpu()\n",
        "    preds = preds.data.view(-1).numpy()\n",
        "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
        "    #labels = [dataset.pos_vocab.id2word(x) for x in labels]\n",
        "    #sentence = [test_dataset.token_vocab.id2word(x) for x in ids]\n",
        "    for word, tag, label in zip(sentence, pos_tags, labels):\n",
        "        print('%s - %s - %s' % (word, tag, label))"
      ],
      "metadata": {
        "id": "ntm5GMEVnOIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CoNLLDataset('en_ewt-ud-test.conllu')\n",
        "dataset = CoNLLDataset('en_ewt-ud-train.conllu')\n",
        "\n",
        "sentence, labels = test_dataset[10]\n",
        "sentence = [test_dataset.token_vocab.id2word(x) for x in sentence]\n",
        "print(sentence)\n",
        "labels = [test_dataset.pos_vocab.id2word(x) for x in labels]\n",
        "inference_with_labels(sentence, labels)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzBnx302k5ob",
        "outputId": "c3e3a41f-8696-4b3d-91e5-ca8fab125ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'doubt', 'the', 'very', 'few', 'who', 'actually', 'read', 'my', 'blog', 'have', 'not', 'come', 'across', 'this', 'yet', ',', 'but', 'I', 'figured', 'I', 'would', 'put', 'it', 'out', 'there', 'anyways', '.']\n",
            "[[9], [1751], [3], [78], [205], [72], [336], [374], [34], [2549], [17], [24], [216], [796], [25], [384], [4], [43], [9], [3755], [9], [48], [197], [15], [55], [58], [3534], [2]]\n",
            "I - DET - PRON\n",
            "doubt - NOUN - VERB\n",
            "the - PUNCT - DET\n",
            "very - NOUN - ADV\n",
            "few - NOUN - ADJ\n",
            "who - PRON - PRON\n",
            "actually - NOUN - ADV\n",
            "read - NOUN - VERB\n",
            "my - PRON - PRON\n",
            "blog - NOUN - NOUN\n",
            "have - PRON - AUX\n",
            "not - PROPN - PART\n",
            "come - ADV - VERB\n",
            "across - AUX - ADP\n",
            "this - PUNCT - PRON\n",
            "yet - NOUN - ADV\n",
            ", - PUNCT - PUNCT\n",
            "but - ADJ - CCONJ\n",
            "I - DET - PRON\n",
            "figured - PRON - VERB\n",
            "I - DET - PRON\n",
            "would - PART - AUX\n",
            "put - SCONJ - VERB\n",
            "it - ADJ - PRON\n",
            "out - VERB - ADV\n",
            "there - ADJ - ADV\n",
            "anyways - VERB - ADV\n",
            ". - DET - PUNCT\n"
          ]
        }
      ]
    }
  ]
}